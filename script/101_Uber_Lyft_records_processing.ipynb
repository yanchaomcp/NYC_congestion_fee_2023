{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Reference Data\n",
    "# ---------------------------\n",
    "\n",
    "HVFHV_dict = {\n",
    "    \"Uber\": [\"B02877\",\"B02866\",\"B02882\",\"B02869\",\"B02617\",\"B02876\",\n",
    "             \"B02865\",\"B02512\",\"B02888\",\"B02864\",\"B02883\",\"B02875\",\n",
    "             \"B02682\",\"B02880\",\"B02870\",\"B02404\",\"B02598\",\"B02765\",\n",
    "             \"B02879\",\"B02867\",\"B02878\",\"B02887\",\"B02872\",\"B02836\",\n",
    "             \"B02884\",\"B02835\",\"B02764\",\"B02889\",\"B02871\",\"B02395\",\n",
    "             \"B03404\"],\n",
    "    \"Lyft\": [\"B02510\",\"B02844\",\"B03406\"],\n",
    "    \"Juno\": [\"B02914\",\"B02907\",\"B02908\",\"B03035\"],\n",
    "    \"Via\":  [\"B03136\",\"B02800\"]\n",
    "}\n",
    "HVFHV_bases = [value for values in HVFHV_dict.values() for value in values]\n",
    "print(\"high-volume fhv base numbers:{}\".format(len(HVFHV_bases)))\n",
    "\n",
    "\n",
    "company_dict = {'HV0002':\"Juno\",'HV0003':\"Uber\",'HV0004':\"Via\",'HV0005':\"Lyft\"}\n",
    "\n",
    "# Taxi zone shapefile: using only Manhattan zones\n",
    "tz = gpd.read_file('data/shapefile/taxiZone/geo_export_bb555bf4-8fc5-4144-b5f6-615889d80884.shp')\n",
    "mh_id = tz[tz.borough == 'Manhattan']['location_i'].unique()\n",
    "\n",
    "# Congestion zone CSV file: used for zone classification later\n",
    "cg = pd.read_csv('data/shapefile/taxiZone/congestZone.csv')\n",
    "cg['location_i'] = cg['location_i'].round(0)\n",
    "c_id = cg['location_i'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90560a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------\n",
    "\n",
    "def process_time(df):\n",
    "    \"\"\"\n",
    "    Standardize time columns, extract temporal features, and compute trip duration and speed.\n",
    "    Uses either 'dropOff_datetime' or 'dropoff_datetime' based on available columns.\n",
    "    Speed is calculated using 'trip_distance' if present; otherwise, 'trip_miles' is used.\n",
    "    \"\"\"\n",
    "    # Rename time columns based on available names\n",
    "    if 'dropOff_datetime' in df.columns:\n",
    "        df = df.rename(columns={\n",
    "            'pickup_datetime': 'PU_time', \n",
    "            'dropOff_datetime': 'DO_time',\n",
    "            'PUlocationID': 'PULocationID', \n",
    "            'DOlocationID': 'DOLocationID'\n",
    "        })\n",
    "    elif 'dropoff_datetime' in df.columns:\n",
    "        df = df.rename(columns={'pickup_datetime': 'PU_time', 'dropoff_datetime': 'DO_time'})\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'dropOff_datetime' nor 'dropoff_datetime' found.\")\n",
    "    \n",
    "    df['PU_time'] = pd.to_datetime(df['PU_time'])\n",
    "    df['DO_time'] = pd.to_datetime(df['DO_time'])\n",
    "    \n",
    "    # Extract time features\n",
    "    df['date'] = df['PU_time'].dt.date\n",
    "    df['year'] = df['PU_time'].dt.year\n",
    "    df['month'] = df['PU_time'].dt.month\n",
    "    df['day'] = df['PU_time'].dt.day\n",
    "    df['dow'] = df['PU_time'].dt.dayofweek\n",
    "    df['dayID'] = df['month'].apply(lambda x: str(x).zfill(2)) + df['day'].apply(lambda x: str(x).zfill(2)) # for future table join\n",
    "    df['pu_hour'] = df['PU_time'].dt.hour\n",
    "    df['pu_min'] = df['PU_time'].dt.minute\n",
    "    df['do_hour'] = df['DO_time'].dt.hour\n",
    "    df['do_min'] = df['DO_time'].dt.minute\n",
    "    df['trip_duration'] = (df['DO_time'] - df['PU_time']).dt.total_seconds()\n",
    "    \n",
    "    # Calculate speed if possible\n",
    "    if 'trip_distance' in df.columns:\n",
    "        df['speed'] = df['trip_distance'] / (df['trip_duration'] / 3600)\n",
    "    elif 'trip_miles' in df.columns:\n",
    "        df['speed'] = df['trip_miles'] / (df['trip_duration'] / 3600)\n",
    "    return df\n",
    "\n",
    "def filter_boundary_and_time(df, boundary_id):\n",
    "    \"\"\"\n",
    "    Filter trips within the specified spatial boundary and restrict to weekdays.\n",
    "    \"\"\"\n",
    "#     ini_shape = df.shape[0]\n",
    "    df = df[df.PULocationID.isin(boundary_id) & df.DOLocationID.isin(boundary_id)]\n",
    "#     print(\"After boundary filter: {:.2f}\".format(df.shape[0] / ini_shape))\n",
    "    df = df[df.dow <= 4]\n",
    "#     print(\"After weekday filter: {:.2f}\".format(df.shape[0] / ini_shape))\n",
    "    return df\n",
    "\n",
    "def filter_duration(df, min_duration=5*60, max_duration=100*60):\n",
    "    \"\"\"\n",
    "    Filter trips based on trip duration.\n",
    "    \"\"\"\n",
    "#     ini_shape = df.shape[0]\n",
    "    df = df[(df.trip_duration >= min_duration) & (df.trip_duration <= max_duration)]\n",
    "#     print(\"After duration filter: {:.2f}\".format(df.shape[0] / ini_shape))\n",
    "    return df\n",
    "\n",
    "def basic_numeric_check(df):\n",
    "    \"\"\"\n",
    "    Perform basic numeric validations for FHV records.\n",
    "    Checks require positive values for trip_miles, speed, base_passenger_fare, and driver_pay.\n",
    "    \"\"\"\n",
    "#     ini_shape = df.shape[0]\n",
    "    condition = (df.trip_miles > 0) & (df.trip_duration > 0) & (df.speed > 0) & \\\n",
    "                (df.base_passenger_fare > 0) & (df.driver_pay > 0)\n",
    "    df = df[condition]\n",
    "#     print(\"After numeric check: {:.2f}\".format(df.shape[0] / ini_shape))\n",
    "    return df\n",
    "\n",
    "\n",
    "def sanity_check(df):\n",
    "    \"\"\"\n",
    "    Apply sanity checks on numeric columns for FHV records.\n",
    "    \"\"\"\n",
    "#     ini_shape = df.shape[0]\n",
    "    condition = (df.base_passenger_fare >= 2.5) & (df.base_passenger_fare <= 300) & \\\n",
    "                (df.driver_pay >= 2) & (df.driver_pay <= 300) & \\\n",
    "                (df.trip_miles >= df.trip_miles.quantile(0.01)) & (df.trip_miles <= 100) & \\\n",
    "                (df.speed >= df.speed.quantile(0.01)) & (df.speed <= 80) & \\\n",
    "                (df.trip_duration >= 5*60) & (df.trip_duration <= 100*60)\n",
    "    df = df[condition]\n",
    "#     print(\"After sanity check: {:.2f}\".format(df.shape[0] / ini_shape))\n",
    "    return df\n",
    "\n",
    "def zones_check(df):\n",
    "    \"\"\"\n",
    "    Classify trips based on whether their pickup and drop-off locations fall within congested regions.\n",
    "    Refer to the accompanying paper for further details.\n",
    "    \"\"\"\n",
    "    zones_conditions = [\n",
    "        (df.PULocationID.isin(c_id)) & (df.DOLocationID.isin(c_id)),\n",
    "        (df.PULocationID.isin(c_id)) & (~df.DOLocationID.isin(c_id)),\n",
    "        (~df.PULocationID.isin(c_id)) & (df.DOLocationID.isin(c_id)),\n",
    "        (~df.PULocationID.isin(c_id)) & (~df.DOLocationID.isin(c_id))\n",
    "    ]\n",
    "    zones = ['aa', 'ab', 'ba', 'bb']\n",
    "    df['zones'] = np.select(zones_conditions, zones)\n",
    "#     print(\"Zone distribution:\")\n",
    "#     print(df['zones'].value_counts(normalize=True))\n",
    "    return df\n",
    "\n",
    "def time_filter_month(df, month):\n",
    "    \"\"\"\n",
    "    Filter DataFrame to include only records within the specified month.\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(f\"{month}-01\")\n",
    "    end_date = start_date + pd.offsets.MonthBegin(1)\n",
    "    return df[(df.PU_time >= start_date) & (df.PU_time < end_date)]\n",
    "\n",
    "def identify_hvfhv(df, HVFHV_dict,  method=\"dispatching\"):\n",
    "    \"\"\"\n",
    "    Identify high-volume FHV companies based on base numbers.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with FHV records.\n",
    "    - HVFHV_dict: Dictionary mapping company names to base numbers.\n",
    "    - method: 'dispatching' (default) or 'affiliated' to choose the column for identification.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame filtered to only include rows identified as hv-fhv.\n",
    "    \"\"\"\n",
    "    df['hvfhv'] = np.nan\n",
    "    for key, bases in HVFHV_dict.items():\n",
    "        if method == \"dispatching\":\n",
    "            df.loc[df['dispatching_base_num'].isin(bases), 'hvfhv'] = key # adopt dispatch according to TLC's reply\n",
    "        else:\n",
    "            df.loc[df['Affiliated_base_number'].isin(bases), 'hvfhv'] = key\n",
    "    \n",
    "    return df.dropna(subset=['hvfhv'])\n",
    "\n",
    "def aggregate_pickups(df, company, group_cols=None):\n",
    "    \"\"\"\n",
    "    Aggregate hourly pick-up counts for a specific company.\n",
    "    \"\"\"\n",
    "    if group_cols is None:\n",
    "        group_cols = ['date', 'year', 'dayID', 'pu_hour', 'PULocationID', 'dow', 'zones']\n",
    "    sdf = df[df.hvfhv == company]\n",
    "    agg = sdf.groupby(group_cols, as_index=False).agg(PUn_trips=('DOLocationID', 'count'))\n",
    "    agg.rename(columns={'PULocationID': 'locID'}, inplace=True)\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f4944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Main Processing Pipeline for FHV Records\n",
    "# ---------------------------\n",
    "\n",
    "def process_fhv_record(name, parquet_read = True, hvfhv_identity = True, filtration = True, hvfhv_method=\"dispatching\"):\n",
    "    \"\"\"\n",
    "    Process FHV data for a given month.\n",
    "    \n",
    "    Parameters:\n",
    "    - name: Month string in format 'YYYY-MM'.\n",
    "    - parquet_read: deal with specific request for some months' records.\n",
    "    - hvfhv_identify: Whether to identify high-volume FHV companies.\n",
    "    - filtration: specific request for some months' records\n",
    "    - hvfhv_method: Column selection for identification ('dispatching' or 'affiliated').\n",
    "    \n",
    "    Returns:\n",
    "    Cleaned DataFrame.\n",
    "    \"\"\" \n",
    "    \n",
    "    if parquet_read:\n",
    "        input_path = f\"data/fhv/fhv_tripdata_{name}.parquet\"\n",
    "        df = pd.read_parquet(input_path)\n",
    "    else:\n",
    "        if name in ['2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07']:\n",
    "            input_path = f\"data/fhv/fhvhv_tripdata_{name}.parquet\"\n",
    "            df = pq.read_table(input_path).to_pandas()\n",
    "        else:\n",
    "            input_path = f\"data/fhv/fhv_tripdata_{name}.parquet\"\n",
    "            table = pq.read_table(input_path)\n",
    "            df = table.filter(\n",
    "                pc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n",
    "                ).to_pandas()\n",
    " \n",
    "    print(f\"Processing FHV data for {name}. Initial shape: {df.shape}\")\n",
    "    \n",
    "    # Identify hv-fhv if required\n",
    "    if hvfhv_identity:\n",
    "        df = identify_hvfhv(df, HVFHV_dict, method=hvfhv_method)\n",
    "    else:\n",
    "        df['hvfhv'] = df['hvfhs_license_num'].map(company_dict)\n",
    "#     print(\"After hv-fhv identification, shape:\", df.shape)\n",
    "\n",
    "    # Process time columns and extract features\n",
    "    df = process_time(df)\n",
    "    \n",
    "    # Apply boundary and weekday filtering\n",
    "    df = filter_boundary_and_time(df, mh_id)\n",
    "    \n",
    "    if filtration:\n",
    "        # Apply cleaning pipeline\n",
    "        df = basic_numeric_check(df)\n",
    "        df = sanity_check(df)\n",
    "    \n",
    "    # Classify trips by congestion zones\n",
    "    df = zones_check(df)\n",
    "    \n",
    "    # Time filter: keep records only within the specified month\n",
    "    df = time_filter_month(df, name)\n",
    "    \n",
    "    # Save cleaned data\n",
    "    output_dir = \"data/fhv/cleaned_hvfhv\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"fhvhv_tripdata_{name}.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "#     print(f\"Cleaned data saved to {output_path}. Final shape: {df.shape}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Process and Aggregate\n",
    "# ---------------------------\n",
    "\n",
    "# may take 20~40 mins to proceed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    for month_to_process in ['2017-08', '2017-09', '2017-10', '2017-11', '2017-12','2018-01','2018-02', '2018-03', '2018-04']:\n",
    "        # Select file_format and cleaning_pipeline based on dataset specifics.\n",
    "        cleaned_df = process_fhv_record(month_to_process, parquet_read = True, \\\n",
    "                                        hvfhv_identity = True, filtration = False, hvfhv_method=\"dispatching\")\n",
    "        \n",
    "        for key in ['Uber','Lyft']:\n",
    "            company_key = key\n",
    "            agg_df = aggregate_pickups(cleaned_df, company_key)\n",
    "            print(\"Total pick-ups aggregated for\", company_key, \":\", agg_df['PUn_trips'].sum())\n",
    "\n",
    "            # Save aggregated data\n",
    "            agg_dir = f\"data/fhv/agg_hvfhv/{company_key}\"\n",
    "            os.makedirs(agg_dir, exist_ok=True)\n",
    "            agg_output_path = os.path.join(agg_dir, f\"{company_key}_aggtrip_{month_to_process}.csv\")\n",
    "            agg_df.to_csv(agg_output_path, index=False)\n",
    "            print(f\"Aggregated data saved to {agg_output_path}.\")\n",
    "        \n",
    "    for month_to_process in [\"2018-05\", \"2018-06\", \"2018-07\", \"2018-08\", \"2018-09\", \"2018-10\", \"2018-11\", \"2018-12\", \"2019-01\"]:\n",
    "        # Select file_format and cleaning_pipeline based on dataset specifics.\n",
    "        cleaned_df = process_fhv_record(month_to_process, parquet_read = False, \\\n",
    "                                        hvfhv_identity = True, filtration = False, hvfhv_method=\"dispatching\")\n",
    "        \n",
    "        for key in ['Uber','Lyft']:\n",
    "            company_key = key\n",
    "            agg_df = aggregate_pickups(cleaned_df, company_key)\n",
    "            print(\"Total pick-ups aggregated for\", company_key, \":\", agg_df['PUn_trips'].sum())\n",
    "\n",
    "            # Save aggregated data\n",
    "            agg_dir = f\"data/fhv/agg_hvfhv/{company_key}\"\n",
    "            os.makedirs(agg_dir, exist_ok=True)\n",
    "            agg_output_path = os.path.join(agg_dir, f\"{company_key}_aggtrip_{month_to_process}.csv\")\n",
    "            agg_df.to_csv(agg_output_path, index=False)\n",
    "            print(f\"Aggregated data saved to {agg_output_path}.\")\n",
    "        \n",
    "    for month_to_process in ['2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07']:\n",
    "        # Select file_format and cleaning_pipeline based on dataset specifics.\n",
    "        cleaned_df = process_fhv_record(month_to_process, parquet_read = False, \\\n",
    "                                        hvfhv_identity = False, filtration = True, hvfhv_method=\"dispatching\")\n",
    "        \n",
    "        for key in ['Uber','Lyft']:\n",
    "            company_key = key\n",
    "            agg_df = aggregate_pickups(cleaned_df, company_key)\n",
    "            print(\"Total pick-ups aggregated for\", company_key, \":\", agg_df['PUn_trips'].sum())\n",
    "\n",
    "            # Save aggregated data\n",
    "            agg_dir = f\"data/fhv/agg_hvfhv/{company_key}\"\n",
    "            os.makedirs(agg_dir, exist_ok=True)\n",
    "            agg_output_path = os.path.join(agg_dir, f\"{company_key}_aggtrip_{month_to_process}.csv\")\n",
    "            agg_df.to_csv(agg_output_path, index=False)\n",
    "            print(f\"Aggregated data saved to {agg_output_path}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7084c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee34649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba5b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394268da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6763c587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8774cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ccd5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e4ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee23bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dcf240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3c16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c18b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019dbe4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
