{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1790fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Configure warnings and plotting\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Load Reference Data\n",
    "# Make sure you downloaded the data/shapefile folder before run the code\n",
    "# ---------------------------\n",
    "\n",
    "# Taxi zone shapefile: using only Manhattan zones for the study region\n",
    "tz = gpd.read_file('data/shapefile/taxiZone/geo_export_bb555bf4-8fc5-4144-b5f6-615889d80884.shp')\n",
    "mh_id = tz[tz.borough == 'Manhattan']['location_i'].unique()\n",
    "\n",
    "# Congestion zone CSV file: used for zone classification later\n",
    "cg = pd.read_csv('data/shapefile/taxiZone/congestZone.csv')\n",
    "cg['location_i'] = cg['location_i'].round(0)\n",
    "c_id = cg['location_i'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bfcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Define Cleaning Functions\n",
    "# ---------------------------\n",
    "\n",
    "def time_check(df):\n",
    "    \"\"\"\n",
    "    Process datetime columns and extract various time features.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Raw taxi trip records.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: With new time-related columns.\n",
    "    \"\"\"\n",
    "    # rename columns\n",
    "    df = df.rename(columns={'tpep_pickup_datetime': 'PU_time',\n",
    "                            'tpep_dropoff_datetime': 'DO_time'})\n",
    "    df['PU_time'] = pd.to_datetime(df['PU_time'])\n",
    "    df['DO_time'] = pd.to_datetime(df['DO_time'])\n",
    "    \n",
    "    # Extract date and time features\n",
    "    df['date'] = df['PU_time'].dt.date\n",
    "    df['year'] = df['PU_time'].dt.year\n",
    "    df['month'] = df['PU_time'].dt.month\n",
    "    df['day'] = df['PU_time'].dt.day\n",
    "    df['dow'] = df['PU_time'].dt.dayofweek\n",
    "    df['dayID'] = df['month'].apply(lambda x: str(x).zfill(2)) + df['day'].apply(lambda x: str(x).zfill(2)) # for future table join\n",
    "    df['pu_hour'] = df['PU_time'].dt.hour\n",
    "    df['pu_min'] = df['PU_time'].dt.minute\n",
    "    df['do_hour'] = df['DO_time'].dt.hour\n",
    "    df['do_min'] = df['DO_time'].dt.minute\n",
    "    \n",
    "    # Compute trip duration in seconds and speed in miles/hour\n",
    "    df['trip_duration'] = (df['DO_time'] - df['PU_time']).dt.total_seconds()\n",
    "    df['speed'] = df['trip_distance'] / (df['trip_duration'] / 3600)\n",
    "    return df\n",
    "\n",
    "def zones_check(df):\n",
    "    \"\"\"\n",
    "    Classify trips based on whether their pickup and drop-off locations fall within congested regions. \n",
    "    Refer to the paper for further details. \n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Taxi trip records.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: With an added 'zones' column indicating zone combinations.\n",
    "    \"\"\"\n",
    "    zones_conditions = [\n",
    "        (df.PULocationID.isin(c_id)) & (df.DOLocationID.isin(c_id)),\n",
    "        (df.PULocationID.isin(c_id)) & (~df.DOLocationID.isin(c_id)),\n",
    "        (~df.PULocationID.isin(c_id)) & (df.DOLocationID.isin(c_id)),\n",
    "        (~df.PULocationID.isin(c_id)) & (~df.DOLocationID.isin(c_id))\n",
    "    ]\n",
    "    zones = ['aa', 'ab', 'ba', 'bb']\n",
    "    df['zones'] = np.select(zones_conditions, zones)\n",
    "    return df\n",
    "\n",
    "def basic_check(df, boundary_id):\n",
    "    \"\"\"\n",
    "    Filter trips by spatial boundary, weekday, and basic validity of values.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Taxi trip records.\n",
    "        boundary_id (array-like): Valid zone IDs (e.g., Manhattan).\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Filtered taxi trip records.\n",
    "    \"\"\"\n",
    "    # Filter trips within the boundary for both pickup and drop-off\n",
    "    df = df[df.PULocationID.isin(boundary_id) & df.DOLocationID.isin(boundary_id)]\n",
    "    \n",
    "    # Keep only weekday trips (Monday-Friday; dow: 0-4)\n",
    "    df = df[df.dow <= 4]\n",
    "    \n",
    "    # Remove records with non-positive values\n",
    "    valid = (df.trip_distance > 0) & (df.trip_duration > 0) & (df.speed > 0) & \\\n",
    "            (df.fare_amount > 0) & (df.total_amount > 0)\n",
    "    df = df[valid]\n",
    "    return df\n",
    "\n",
    "def sanity_check(df):\n",
    "    \"\"\"\n",
    "    Apply further sanity checks on fare, distance, speed, and duration.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): Taxi trip records.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Taxi trip records passing sanity checks.\n",
    "    \"\"\"\n",
    "    condition = (\n",
    "        (df.fare_amount >= 2.5) & (df.fare_amount <= 300) &\n",
    "        (df.trip_distance >= df.trip_distance.quantile(0.01)) & (df.trip_distance <= 100) &\n",
    "        (df.speed >= df.speed.quantile(0.01)) & (df.speed <= 80) &\n",
    "        (df.trip_duration >= 5*60) & (df.trip_duration <= 100*60)\n",
    "    )\n",
    "    df = df[condition]\n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# Wrapper Function for Processing and Aggregation\n",
    "# ---------------------------\n",
    "\n",
    "def clean_and_aggregate(month, \n",
    "                        input_dir=\"data/taxi\", \n",
    "                        cleaned_dir=\"data/taxi/cleaned_taxi\", \n",
    "                        agg_dir=\"data/taxi/agg_yellow\"):\n",
    "    \"\"\"\n",
    "    Clean taxi trip data for a given month, aggregate hourly pick-up counts,\n",
    "    and save both cleaned and aggregated data to CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "        month (str): Month string in the format 'YYYY-MM'.\n",
    "        input_dir (str): Directory containing raw taxi data in parquet format.\n",
    "        cleaned_dir (str): Directory to save cleaned taxi data.\n",
    "        agg_dir (str): Directory to save aggregated taxi data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct file paths\n",
    "    file_base = f\"{input_dir}/yellow_tripdata\"\n",
    "    file_path = f\"{file_base}_{month}.parquet\"\n",
    "    \n",
    "    # Load raw data\n",
    "    print(f\"Processing month: {month}\")\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Apply cleaning functions sequentially\n",
    "    df = time_check(df)\n",
    "    df = basic_check(df, boundary_id=mh_id)\n",
    "    df = sanity_check(df)\n",
    "    df = zones_check(df)\n",
    "    \n",
    "    # Furthr filter the DataFrame to include trips within the specified month\n",
    "    start_date = pd.to_datetime(f\"{month}-01\")\n",
    "    end_date = start_date + pd.offsets.MonthBegin(1)  # start of the next month\n",
    "    df = df[(df.PU_time >= start_date) & (df.PU_time < end_date)]\n",
    "\n",
    "    \n",
    "    # Save cleaned data\n",
    "    os.makedirs(cleaned_dir, exist_ok=True)\n",
    "    cleaned_path = os.path.join(cleaned_dir, f\"yellow_tripdata_{month}.csv\")\n",
    "    df.to_csv(cleaned_path, index=False)\n",
    "    \n",
    "    # Aggregate data: count hourly pick-ups per taxi zone and sum fare amounts\n",
    "    agg = df.groupby(['date', 'year', 'dayID', 'pu_hour', 'PULocationID', 'dow', 'zones'], \n",
    "                     as_index=False).agg(PUn_trips=('DOLocationID', 'count'),\n",
    "                                          fare_amount=('fare_amount', 'sum'))\n",
    "    # Rename for clarity\n",
    "    agg.rename(columns={'PULocationID': 'locID'}, inplace=True)\n",
    "    \n",
    "    # Save aggregated data\n",
    "    os.makedirs(agg_dir, exist_ok=True)\n",
    "    agg_path = os.path.join(agg_dir, f\"yellow_aggtrip_{month}.csv\")\n",
    "    agg.to_csv(agg_path, index=False)\n",
    "    \n",
    "    print(f\"Finished processing month: {month}. Cleaned and aggregated files saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180449bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Main Loop: Process All Months\n",
    "# ---------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of months to process \n",
    "    months = [\n",
    "        '2017-08', '2017-09', '2017-10', '2017-11', '2017-12',\n",
    "        '2018-01', '2018-02', '2018-03', '2018-04', '2018-05', '2018-06', \n",
    "        '2018-07', '2018-08', '2018-09', '2018-10', '2018-11', '2018-12', \n",
    "        '2019-01', '2019-02', '2019-03', '2019-04', '2019-05', '2019-06', '2019-07'\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Loop over each month and process the data\n",
    "    # may take 15~30 mins to proceed\n",
    "    for month in months:\n",
    "        clean_and_aggregate(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e10771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018805b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc9015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d960bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673f5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef959fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f11a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16b4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e92db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba349e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
